{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e1c66b",
   "metadata": {},
   "source": [
    "# Logistic Regression Metrics and its Formuals & Calculations:\n",
    "# =============================================================\n",
    "\n",
    "\n",
    "Below are the Metrics(comes from classification_report) to evaluate the Logistic Regression model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# True Positive Rate (Sensitivity or Recall of +ve class):\n",
    "\n",
    "This is the proportion of actual positive instances correctly identified by the model. It is calculated as the number    of true positives divided by the sum of true positives and false negatives.\n",
    "\n",
    "             Sensitivity= TP/(TP + FN)\n",
    "        \n",
    "# Precision:\n",
    "\n",
    "Precesion for Positive class: It is the proportion of Actual Positives among those predicted as positive(both including TP and FP). It tells us how accurate our Positive class predictions are.\n",
    "\n",
    "             Precision(for +ve class) = TP(TP + FP)\n",
    "\n",
    "        \n",
    "Precesion for Negative class: It is the proportion of Actual Negatives among those predicted as Negative(both including TN and FN). It tells us how accurate our Negative class predictions are.       \n",
    "                  \n",
    "             precision(for -ve class) = TN/(TN + FN)\n",
    "      \n",
    "\n",
    "# False Positive Rate(Type I Error): \n",
    "    \n",
    "This is the proportion of actual negative instances incorrectly classified as positive by the model. It is calculated by the number of false positives divided by the sum of false positives and true negatives.\n",
    "\n",
    "             False Positive Rate= FP/(FP + TN) or \n",
    "\n",
    "             False Positive Rate= (1 - Specificity)\n",
    "\n",
    "# False Negative Rate(Type II Error):\n",
    "\n",
    "This is the proportion of actual positive instaces incorrectly classified as negative class by the model. Its caluclated  by the number of false negative divided by the sum of false negatives and troe positives.\n",
    "\n",
    "              False Negative Rate= FN/(FN +TP)\n",
    "\n",
    "\n",
    "# True Negative Rate(Specificity or Recall of Negative class): \n",
    "      \n",
    "This is the complement of the false positive rate and represents the proportion of actual negative instances correctly     identified by the model.\n",
    "\n",
    "              Specificity = (1 - False Positive Rate) or\n",
    "                 \n",
    "              True Negative Rate or Specificity=  TN/ (TN + FP)\n",
    "                  \n",
    "                  \n",
    "# F1 Score: \n",
    "\n",
    "It is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "              F1 Score(+ve or -ve classes) =  (2 × Precision × Recall)\n",
    "                                              ---------------------------\n",
    "                                                  (Precision + Recall)\n",
    "    \n",
    "\n",
    "# Accuracy:\n",
    "\n",
    "This is the proprotion of Correct predictions to the total number of predictions.\n",
    "\n",
    "              Accuracy= (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "\n",
    "# Support: \n",
    "\n",
    "Total number of data points in each class.\n",
    "\n",
    "\n",
    "# Macro Average:\n",
    "\n",
    "This is calculates the Average of Metrics(Precision, Recall & F1-score) across all classes by giving Equal Weights to each class\n",
    " \n",
    "    Binary Class:\n",
    "    -------------\n",
    "            \n",
    "          Macro Avg of Precision = (Precision of Class_0 + Precision of Class_1)/2\n",
    "\n",
    "          Macro Avg of Recall = (Recall of class_0 + Recall of Class_1)/2\n",
    "\n",
    "          Macro avg of F1-Score= (F1-Score of class_0 + F1-score of class_1)/2\n",
    "            \n",
    "    Multiclass:\n",
    "    ------------\n",
    "    \n",
    "          Macro Avg of Precision = (Precision of Class_0 + Precision of Class_1+ ...Precision of class_n)/n\n",
    "\n",
    "          Macro Avg of Recall = (Recall of class_0 + Recall of Class_1+ ... Recall of class_n)/n\n",
    "\n",
    "          Macro avg of F1-Score= (F1-Score of class_0 + F1-score of class_1+ ...F1-score of class_n)/n\n",
    "            \n",
    "\n",
    "# Weighted Average:\n",
    "\n",
    "This calculates the Average of Metrics(Precision, Recall & F1-score), weighted by the support of each class. This gives the more importance to the classes those having more datapoints.\n",
    "\n",
    "\n",
    "Note: Assuming Binary Class problem.\n",
    "\n",
    "    Calculate the Precision, Recall & F1-score for the +ve class:\n",
    "    ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                 Precision of +ve class(class_0 lets say) =   TP(TP + FP)\n",
    "                 \n",
    "                 Recall of +ve class(class_0) =  TP/(TP + FN)\n",
    "                 (aka TPR or Sensitivity)\n",
    "                 \n",
    "                 F1-score of +ve class(class_0) = (2 × Precision of +ve × Recall of +ve)\n",
    "                                                 -----------------------------------------\n",
    "                                                      (Precisionof +ve + Recall +ve)\n",
    "                 \n",
    "\n",
    "    Calculate the Precision, Recall & F1-score for the -ve class:\n",
    "    ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "                 Precision of -ve class(class_1 lets say) =   TN(TN + FN)\n",
    "                 \n",
    "                 Recall of -ve class(class_1) = TN/(TN + FP)\n",
    "                 (aka TNR or Specificity)      \n",
    "                 \n",
    "                 F1-score of -ve class(class_1)=  (2 × Precision of -ve × Recall of -ve)\n",
    "                                                  ------------------------------------------\n",
    "                                                       (Precisionof -ve + Recall -ve)\n",
    "                 \n",
    "    \n",
    "    Calculate the Weighted Avg of Precison, Recall & F1-score:\n",
    "    ----------------------------------------------------------\n",
    "\n",
    "\n",
    "       Weighted Avg of Precision = \n",
    "                \n",
    "              precision(class_0) * (support of class_0) + precision(class_1) * (support of class_1)\n",
    "              -------------------------------------------------------------------------------------\n",
    "                                   (support of class_0 + support of class_1 )\n",
    "                                                    \n",
    "       \n",
    "       Weighted Avg of Recall =  \n",
    "       \n",
    "               Recall(class_0) * (support of class_0) + Recall(class_1) * (support of class_1)\n",
    "               -------------------------------------------------------------------------------------\n",
    "                                    (support of class_0 + support of class_1 )\n",
    "                                                    \n",
    "                                                    \n",
    "       Weighted Avg of F1-score = \n",
    "       \n",
    "               F1-score(class_0) * (support of class_0) + F1-score(class_1) * (support of class_1)\n",
    "               -------------------------------------------------------------------------------------\n",
    "                                     (support of class_0 + support of class_1 )\n",
    "    \n",
    "                  \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32305c64",
   "metadata": {},
   "source": [
    "# Different Solvers in Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055afee",
   "metadata": {},
   "source": [
    "# 1. lbfgs (Limited-memory Broyden-Fletcher-Goldfarb-Shanno):\n",
    "\n",
    "    Algorithm: Quasi-Newton method.\n",
    "    \n",
    "    Strengths: Fast convergence for medium-sized datasets, supports only L2 regularization. Handles multi-class problems with cross-entropy loss.\n",
    "    \n",
    "    Weaknesses: May not be suitable for very large datasets, memory usage can be higher than other solvers.\n",
    "    \n",
    "    Use cases: General-purpose choice for diverse datasets, especially with multi-class classification.\n",
    "\n",
    "\n",
    "# 2. liblinear:\n",
    "\n",
    "    Algorithm: Coordinate descent.\n",
    "    \n",
    "    Strengths: Fast for small to medium datasets with sparse features, supports L1 and L2 regularization.\n",
    "    \n",
    "    Weaknesses: Not suitable for large datasets, can be slow for dense features, and only supports one-vs-rest multi-class           classification.\n",
    "    \n",
    "    Use cases: When dealing with small datasets or limited resources, particularly with L1 regularization for feature selection.\n",
    "\n",
    "# 3. sag (Stochastic Average Gradient):\n",
    "\n",
    "    Algorithm: Stochastic gradient descent with averaging.\n",
    "    \n",
    "    Strengths: Highly scalable for large datasets, efficient memory usage, supports both L1 & L2 regularization.\n",
    "    \n",
    "    Weaknesses: Requires more iterations than other solvers, convergence might not be guaranteed for all cases.\n",
    "    \n",
    "    Use cases: Ideal for massive datasets where computational efficiency is crucial.\n",
    "\n",
    "\n",
    "# 4. saga (Stochastic Average Gradient with momentum):\n",
    "\n",
    "    Algorithm: Similar to sag but with momentum term for faster convergence.\n",
    "    \n",
    "    Strengths: Combines benefits of sag with faster convergence, supports both L1 and L2 regularization.\n",
    "    \n",
    "    Weaknesses: Still susceptible to non-convergence in some cases.\n",
    "    \n",
    "    Use cases: Preferred choice for large datasets, especially with L1 regularization for sparsity.\n",
    "\n",
    "\n",
    "# 5. newton-cg (Newton Conjugate Gradient):\n",
    "\n",
    "    Algorithm: Second-order optimization method.\n",
    "    \n",
    "    Strengths: Highly accurate and fast convergence for small to medium datasets, supports both L1 and L2 regularization.\n",
    "    \n",
    "    Weaknesses: Computationally expensive compared to other solvers, not suitable for large datasets.\n",
    "    \n",
    "    Use cases: When highest accuracy is needed for smaller datasets, particularly with L2 regularization.\n",
    "\n",
    "\n",
    "# 6. newton-cholesky (Newton Conjugate Gradient with Cholesky                                                                                                Decomposition):\n",
    "\n",
    "    Algorithm: Variant of newton-cg with efficient Cholesky decomposition for positive definite Hessian matrices.\n",
    "    \n",
    "    Strengths: Faster than newton-cg for certain problems, inherits similar strengths and weaknesses.\n",
    "    \n",
    "    Weaknesses: Similar limitations as newton-cg, may not be necessary for general cases.\n",
    "    \n",
    "    Use cases: Potential alternative to newton-cg for specific problem conditions where Hessian is known to be positive             definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48948225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdacd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802fac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
